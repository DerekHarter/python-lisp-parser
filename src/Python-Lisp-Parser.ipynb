{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python Lisp Parser Working Notes\n",
      "--------------------------------\n",
      "\n",
      "Notebook to document work and tests setting up a simple Python Lisp Parser.\n",
      "\n",
      "First approach, break into subfunctions to handle parsing a list, and to handle gathering the operator and operands\n",
      "of a list.  This becomes recursive because the function gathering operands will call the list parsing function if\n",
      "it encounters an opening `(` indicating a new subexpression.\n",
      "\n",
      "Here we kind of use half-remembered compiler/parser ideas and language.  We use Python list to represent the resulting\n",
      "parse tree (as suggested).  Structure is a simple list of embedded lists mirroring the lisp structure.  We use the\n",
      "pop(0) on python lists to consume tokens, and we simply examine tokens[0] to peek at next token if we don't want to \n",
      "consume it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(txt):\n",
      "    \"\"\"Tokenize a Lisp-like program\n",
      "    \n",
      "    Break up a program of ascii text on tokens and return a list of the tokens.  This function\n",
      "    considers any sequence of whitespace and the ( and ) as the only valid delimeters between\n",
      "    tokens, which should be good enough for now.  We use Python re library to split up based on our\n",
      "    small list of delimiters.\n",
      "    \n",
      "    Parameters\n",
      "    -----------\n",
      "    txt : string\n",
      "        An ascii string of lisp text to be tokenized\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    tokens : list\n",
      "        A list of the recognized tokens in the text\n",
      "    \"\"\"\n",
      "    # split into tokens, use whitespace or the ( or ) character as delimeters\n",
      "    # the following usage of re.split also returns the delimiters that were matched\n",
      "    tokens = re.split('(\\s+|\\(|\\))', txt)\n",
      "    \n",
      "    # the previous re returns all delimiters, including string of whitespace and empty matches.\n",
      "    # we remove all empty or whitespace only matches, which leaves only valid tokens\n",
      "    return [t for t in tokens if len(t) and not t.isspace()]\n",
      "\n",
      "\n",
      "def parse(txt):\n",
      "    \"\"\"Parse a Lisp-like program\n",
      "    \n",
      "    Parse a program/string of Lisp-like ascii text.  This function is an interface to the\n",
      "    main recursive parsing routines.  This function expects a raw python string of ascii\n",
      "    text.  It first attempts to tokenize the string using blank space and the '(' and ')'\n",
      "    characters as delimiters.  Given this list of tokens, it then attempts to parse the\n",
      "    list into an abstract syntax tree.  This function users helper functions to do the\n",
      "    actual tokenization and parsing tasks.  This function expects a correctly formatted\n",
      "    lisp program, and it checks that the resulting AST is well formatted and that no tokens\n",
      "    are left over after the parse.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    txt : string\n",
      "        An ascii string of lisp text to be parsed\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ast : list (of python lists)\n",
      "        An abstract syntax tree, the result of parsing the Lisp-like code into operator/operand\n",
      "        lists, ready for interpretation.\n",
      "    \"\"\"\n",
      "    # tokenize the text\n",
      "    tokens = tokenize(txt)\n",
      "    \n",
      "    # attempt to parse the tokens\n",
      "    ast, tokens = parse_list(tokens)\n",
      "    \n",
      "    # check if all tokens consumed\n",
      "    if len(tokens) > 0:\n",
      "        raise SyntaxError(\"(parse) Error: not all tokens consumed <%s>\" % str(tokens))\n",
      "        \n",
      "    # return the result\n",
      "    return ast\n",
      "\n",
      "    \n",
      "def parse_list(tokens):\n",
      "    \"\"\"Parse a List\n",
      "    \n",
      "    Consume a (operator operand1 operand2 operand3 ...) expression.  Syntatically\n",
      "    the opening '(' is always followed by an operator, and then a list of at least 1 or\n",
      "    up to many operands.  This function consumes the opening '(' and the operator and\n",
      "    then calls another function to get the list of operands.  When the operands are\n",
      "    gathered, this funciton expects and consumes the closing ')'.  The resulting \n",
      "    parse is put into a list, and the list and any remaining tokens are returned by\n",
      "    this function.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    tokens : list\n",
      "        A python list of valid tokens.  This function expects the first token to be the '('\n",
      "        keyword, and the second token will be an operator.\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    ast : list (of python lists)\n",
      "        Return the resulting abstract syntax tree as a list of lists\n",
      "    tokens : list of strings\n",
      "        Also any remaining tokens after parsing the current list and operands are returned\n",
      "        (to be used for further parsing).\n",
      "    \"\"\"\n",
      "    # expect '(' always as first token for this function\n",
      "    if len(tokens) == 0 or tokens[0] != '(':\n",
      "        raise SyntaxError(\"(parse_list) Error: expected '(' token, found <%s>\" % str(tokens))\n",
      "    first = tokens.pop(0) # consume the opening '('\n",
      "\n",
      "    # consume the operator and all operands\n",
      "    operator = tokens.pop(0) # operator always after opening ( syntatically\n",
      "    operands, tokens = parse_operands(tokens)\n",
      "    ast = [operator]\n",
      "    ast.extend(operands)\n",
      "    \n",
      "    # consume the matching ')'\n",
      "    if len(tokens) == 0 or tokens[0] != ')':\n",
      "        raise SyntaxError(\"(parse_list) Error: expected ')' token, found <%s>: \" % str(tokens))\n",
      "    first = tokens.pop(0) \n",
      "        \n",
      "    return ast, tokens\n",
      "\n",
      "\n",
      "def parse_operands(tokens):\n",
      "    \"\"\"Consume a sequence of operands\n",
      "    \n",
      "    We consume all of the operands of a Lisp like stream of tokens.  We keep going till\n",
      "    there are no tokens left to consume, or we reach a closing ')' expression.  In addition\n",
      "    this function will recursively call parse_list() if it sees an opening '(', in order to\n",
      "    get the sub AST parsed expression.  Syntatically a sub AST is simply an additional \n",
      "    operand of the current list of operands, so if one is found it is just appended to the\n",
      "    list, and we then continue on trying to parse other operands.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    tokens : list\n",
      "        A python list of valid tokens.  \n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    ast : list\n",
      "        A partial list of the parsed abstract syntax tree, basically all of the operands we\n",
      "        could consume from the stream of tokens we were given (including any parsed\n",
      "        subexpressions).\n",
      "    tokens : list\n",
      "        A list of token strings.  The remaining tokens in the parse stream (needed for\n",
      "        further processing).\n",
      "    \"\"\"\n",
      "    operands = []\n",
      "    while len(tokens) > 0:\n",
      "        # peek at next token, and if not an operand then stop\n",
      "        if tokens[0] == ')':\n",
      "            break\n",
      "\n",
      "        # if next token is a '(', need to get sublist/subexpression\n",
      "        if tokens[0] == '(':\n",
      "            subast, tokens = parse_list(tokens)\n",
      "            operands.append(subast)\n",
      "            continue # need to continue trying to see if more operands after the sublist\n",
      "            \n",
      "        # otherwise token must be some sort of an operand\n",
      "        operand = tokens.pop(0) # consume the token and parse it\n",
      "        \n",
      "        # assume we have an operand, try and cast numeric operands, or\n",
      "        # if not numeric simply leave and treat as a string value\n",
      "        operands.append(decode_operand(operand))\n",
      "    \n",
      "    return operands, tokens\n",
      "\n",
      "\n",
      "def decode_operand(token):\n",
      "    \"\"\"Decode a single token string\n",
      "    \n",
      "    Decode a single token string being interpreted syntatically as an operand.  Currently we\n",
      "    try and determine if the token can be interpreted as a float or an int constant.  If it can\n",
      "    not be interpreted as either of these, we default to interpreting the token as a string constant.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    token : string\n",
      "        A python string holding a single token we are trying to decode\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    value : int/float/string\n",
      "        Returns the decoded operand.  The type of the returned value depends solely on the format of\n",
      "        the token being interpreted, and the purpose of the function is to return the best type\n",
      "        given the token.\n",
      "    \"\"\"\n",
      "    if is_int(token):\n",
      "        return int(token)\n",
      "    elif is_float(token):\n",
      "        return float(token)\n",
      "    else: # default to a string\n",
      "        return str(token)\n",
      "\n",
      "    \n",
      "def is_float(s):\n",
      "    \"\"\"Test if float\n",
      "    \n",
      "    Function to test whether given string can be interpreted as a valid floating\n",
      "    point literal value or not.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    s : string\n",
      "        A python string holding a symbol to test\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    result : boolean\n",
      "        True if the string is a valid floating point value, False otherwise\n",
      "    \"\"\"\n",
      "    try:\n",
      "        float(s)\n",
      "        return True\n",
      "    except ValueError:\n",
      "        return False\n",
      "\n",
      "\n",
      "def is_int(s):\n",
      "    \"\"\"Test if int\n",
      "    \n",
      "    Function to test whether given string can be interpreted as a valid integer\n",
      "    literal value or not.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    s : string\n",
      "        A python string holding a symbol to test\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    result : boolean\n",
      "        True if the string is a valid integer, False otherwise\n",
      "    \"\"\"\n",
      "    try:\n",
      "        int(s)\n",
      "        return True\n",
      "    except ValueError:\n",
      "        return False\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# unit tests of the simple Python Lisp Parser\n",
      "import unittest\n",
      "\n",
      "class TestPythonLispParser(unittest.TestCase):\n",
      "    def test_tokenizer_extra_space(self):\n",
      "        self.assertEqual(tokenize(\" (  + apples oranges )  \"), \n",
      "                         ['(', '+', 'apples', 'oranges', ')'])\n",
      "\n",
      "    def test_tokenizer_compact(self):\n",
      "        self.assertEqual(tokenize(\"((+ 1 (2 (3 4))))\"), \n",
      "                         ['(', '(', '+', '1', '(', '2', '(', '3', '4', ')', ')', ')', ')'])\n",
      "\n",
      "    def test_tokenizer_bigger(self):\n",
      "        self.assertEqual(tokenize(\"(first (list 1 (+ 2 3) 9))\"), \n",
      "                         ['(', 'first', '(', 'list', '1', '(', '+', '2', '3', ')', '9', ')', ')'])\n",
      "\n",
      "    def test_is_int_string(self):\n",
      "        self.assertFalse(is_int('hello'))\n",
      "\n",
      "    def test_is_int_valid(self):\n",
      "        self.assertTrue(is_int('42'))\n",
      "\n",
      "    def test_is_int_given_a_float(self):\n",
      "        self.assertFalse(is_int('38.29'))\n",
      "\n",
      "    def test_is_float_string(self):\n",
      "        self.assertFalse(is_float('goodbye'))\n",
      "\n",
      "    def test_is_float_valid(self):\n",
      "        self.assertTrue(is_float('18.76'))\n",
      "        \n",
      "    def test_decode_operand_float(self):\n",
      "        token = '8.8'\n",
      "        val = decode_operand(token)\n",
      "        self.assertTrue(type(val) == float)\n",
      "        self.assertEqual(val, 8.8)\n",
      "        \n",
      "    def test_decode_operand_int(self):\n",
      "        token = '42'\n",
      "        val = decode_operand(token)\n",
      "        self.assertTrue(type(val) == int)\n",
      "        self.assertEqual(val, 42)\n",
      "        \n",
      "    def test_decode_operand_str(self):\n",
      "        token = 'a string'\n",
      "        val = decode_operand(token)\n",
      "        self.assertTrue(type(val) == str)\n",
      "        self.assertEqual(val, 'a string')\n",
      "        \n",
      "    def test_parse_operands_simple(self):\n",
      "        t = tokenize('a 1 2.2')\n",
      "        self.assertEqual(parse_operands(t), (['a', 1, 2.2], []))\n",
      "        \n",
      "    def test_parse_operands_stop_at_close_paren(self):\n",
      "        t = tokenize('1 2 3 )')\n",
      "        self.assertEqual(parse_operands(t), ([1, 2, 3], [')']))\n",
      "        \n",
      "    def test_parse_operands_stop_at_close_paren_more_tokens(self):\n",
      "        t = tokenize('2.2 3.3 a) 9 b')\n",
      "        self.assertEqual(parse_operands(t), ([2.2, 3.3, 'a'], [')', '9', 'b']))\n",
      "        \n",
      "    def test_parse_list_simple(self):\n",
      "        t = tokenize('(+ 2 3)')\n",
      "        self.assertEqual(parse_list(t), (['+', 2, 3], []))\n",
      "        \n",
      "    def test_parse_list_no_opening_paren(self):\n",
      "        t = tokenize('+ 2 3)')\n",
      "        self.assertRaises(SyntaxError, parse_list, t)\n",
      "\n",
      "    def test_parse_list_no_closing_paren(self):\n",
      "        t = tokenize('(+ 2 3')\n",
      "        self.assertRaises(SyntaxError, parse_list, t)\n",
      "        \n",
      "    def test_parse_list_recursive_simple(self):\n",
      "        t = tokenize('(+ 2.2 3 (* 4 5) 1.8 9)')\n",
      "        self.assertEqual(parse_list(t), (['+', 2.2, 3, ['*', 4, 5], 1.8, 9], []))\n",
      "        \n",
      "    def test_parse_list_recursive_complex(self):\n",
      "        t = tokenize('(first (list 1 (+ 2 3) 9))')\n",
      "        self.assertEqual(parse_list(t), (['first', ['list', 1, ['+', 2, 3], 9]], []))\n",
      "    \n",
      "    def test_parse_simple(self):\n",
      "        txt = '(+ 4 3.8 9 a)'\n",
      "        self.assertEqual(parse(txt), ['+', 4, 3.8, 9, 'a'])\n",
      "    \n",
      "    def test_parse_harder(self):\n",
      "        txt = '(first (list 1 (+ 2 3) 9))'\n",
      "        self.assertEqual(parse(txt), ['first', ['list', 1, ['+', 2, 3], 9]])\n",
      "\n",
      "    def test_parse_complex(self):\n",
      "        txt = '(+ 4 (* 3 8 (- 4.8 2) (+ 3 9.5)) 5 (first a b c d (first e f (first g h) i)) 9)'\n",
      "        self.assertEqual(parse(txt),\n",
      "                         ['+', 4, \n",
      "                               ['*', 3, 8, ['-', 4.8, 2], ['+', 3, 9.5]], \n",
      "                               5, \n",
      "                               ['first', 'a', 'b', 'c', 'd', ['first', 'e', 'f', ['first', 'g', 'h'], 'i']], \n",
      "                               9])\n",
      "\n",
      "        \n",
      "suite = unittest.TestLoader().loadTestsFromTestCase(TestPythonLispParser)\n",
      "_ = unittest.TextTestRunner(verbosity=1, stream=sys.stderr).run(suite)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "----------------------------------------------------------------------\n",
        "Ran 22 tests in 0.037s\n",
        "\n",
        "OK\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}