{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python Lisp Parser Working Notes\n",
      "--------------------------------\n",
      "\n",
      "Notebook to document work and tests setting up a simple Python Lisp Parser.\n",
      "\n",
      "First approach, break into subfunctions to handle parsing a list, and to handle gathering the operator and operands\n",
      "of a list.  This becomes recursive because the function gathering operands will call the list parsing function if\n",
      "it encounters an opening `(` indicating a new subexpression.\n",
      "\n",
      "Here we kind of use half-remembered compiler/parser ideas and language.  We use Python list to represent the resulting\n",
      "parse tree (as suggested).  Structure is a simple list of embedded lists mirroring the lisp structure.  We use the\n",
      "pop(0) on python lists to consume tokens, and we simply examine tokens[0] to peek at next token if we don't want to \n",
      "consume it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(txt):\n",
      "    \"\"\"Break up a line of ascii text on tokens and return a list of the tokens.  This function\n",
      "    considers any sequence of whitespace and the ( and ) as the only valid delimeters between\n",
      "    tokens, which should be good enough for now.  We use Python re library to split up based on our\n",
      "    small list of delimiters.\n",
      "    \n",
      "    Parameters\n",
      "    -----------\n",
      "    txt : string\n",
      "        An ascii string of lisp text to be tokenized\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    tokens : list\n",
      "        A list of the recognized tokens in the text\n",
      "    \"\"\"\n",
      "    # split into tokens, use whitespace or the ( or ) character as delimeters\n",
      "    # the following usage of re.split also returns the delimiters that were matched\n",
      "    tokens = re.split('(\\s+|\\(|\\))', txt)\n",
      "    \n",
      "    # the previous re returns all delimiters, including string of whitespace and empty matches.\n",
      "    # we remove all empty or whitespace only matches, which leaves only valid tokens\n",
      "    return [t for t in tokens if len(t) and not t.isspace()]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# unit tests of the simple Python Lisp Parser\n",
      "import unittest\n",
      "\n",
      "class TestPythonLispParser(unittest.TestCase):\n",
      "    \n",
      "    def test_tokenizer_extra_space(self):\n",
      "        self.assertEqual(tokenize(\" (  + apples oranges )  \"), \n",
      "                         ['(', '+', 'apples', 'oranges', ')'])\n",
      "\n",
      "    def test_tokenizer_compact(self):\n",
      "        self.assertEqual(tokenize(\"(+ 1 2 3 4)\"), \n",
      "                         ['(', '+', '1', '2', '3', '4', ')'])\n",
      "\n",
      "    def test_tokenizer_bigger(self):\n",
      "        self.assertEqual(tokenize(\"(first (list 1 (+ 2 3) 9))\"), \n",
      "                         ['(', 'first', '(', 'list', '1', '(', '+', '2', '3', ')', '9', ')', ')'])\n",
      "\n",
      "suite = unittest.TestLoader().loadTestsFromTestCase(TestPythonLispParser)\n",
      "unittest.TextTestRunner(verbosity=1, stream=sys.stderr).run(suite)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "----------------------------------------------------------------------\n",
        "Ran 3 tests in 0.003s\n",
        "\n",
        "OK\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}